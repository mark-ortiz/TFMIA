{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabajo Fin de Master\n",
    "\n",
    "<font size=\"4\">**Universidad Internacional de Valencia**</font>\n",
    "\n",
    "<font size=\"3\">Título: “Aprendizaje profundo aplicado a física de partículas: estimación de trazas de partículas detectadas por el telescopio de neutrinos en el fondo marino.”</font>\n",
    "\n",
    "Máster en Inteligencia Artificial\n",
    "\n",
    "Director TFM: Dr. Salva Ardid\n",
    "\n",
    "Alumno: Ing. Marco Antonio Ortiz Meneses\n",
    "\n",
    "Curso Académico: 2018-2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3><B>CONTROL DE CAMBIOS:</B></font>\n",
    "\n",
    "TFM_v0 - Este archivo contiene el análisis inicial que es erróneo debido a la falta de comprensión de los datos, aunque el preprocesamiento realizado es muy valioso y puede servir.\n",
    "\n",
    "TVM_v0.1 - Este archivo pretende mejorar la limpieza de datos, ya que se cuenta con nuevo conocimiento de los mismos. Adicional se pretende obtener la data a nivel de hit, ya que lo que se obtuvo anteriormente son las \"salidas\", es decir el resultado.\n",
    "\n",
    "Si lo comprendí bien, en general la idea es poder comparar lo que se obtuvo de la trayectoría del muón vs lo que uno puede predecir utilizando aprendizaje profundo, para esto \"tal vez\" hay que comprender mejor que es el bbfit y el afit.\n",
    "\n",
    "**25/Jun**\n",
    "\n",
    "Continúo trabajando con Después de haber finalizado todas las materias y exámenes del Máster, me pongo a este TFM totalmente.\n",
    "\n",
    "Incorporé algunas celdas ya que con base a las instrucciones nuevamente enviadas por mi Director de TFM comprendí mejor lo que debo hacer en el preprocesamiento.\n",
    "\n",
    "**26/Jun**\n",
    "\n",
    "Hoy he comenzado a reordenar los índices para poder \"limpiar\" mejor el archivo y poder localizar la data que realmente se requiere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción\n",
    "\n",
    "Este es el Notebook que utilizaré para el desarrollo del TFM, y en el se contienen los aspectos técnicos relevantes para el tratamiento de los datos motivo del TFM.\n",
    "\n",
    "Como parte fundamental, seguiré la metodología CRISP-DM, la cual conlleva seis fases:\n",
    "* 1. Entendimiento del Negocio (o del Problema en este caso)\n",
    "* 2. Entendimiento de los Datos\n",
    "* 3. Preparación de los Datos\n",
    "* 4. Modelado\n",
    "* 5. Evaluación\n",
    "* 6. Despliegue-Implementación del modelo en producción\n",
    "\n",
    "**Es importante mencionar que los modelos se deben mantener y monitorear, ya que tienden a cambiar con el tiempo y a dejar de ser precisos, sobre todo si las circunstancias originales o variables cambian drásticamente.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ENTENDIMIENTO DEL PROBLEMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el punto 1 de la Metodología, leí los documentos proporcionados por mi Director de TFM y que son:\n",
    "* A fast algorithm for muon track reconstruction and its application to the ANTARES neutrino telescope\n",
    "* Nuclear Instruments and Methods in Physics Research A\n",
    "\n",
    "Una vez entendido a grandes razgos, como es que funciona el telescopio de neutrinos ANTARES y algunos detalles técnicos de operación, procedo a comenzar a analizar los archivos de datos proporcionados.\n",
    "\n",
    "Es importante para mí el mencionar que he leído varias veces los documentos ya que no es fácil entender de primera instancia todas las variables involucradas y sobre todo la física relativa a los neutrinos y otras partículas asociadas a estos.\n",
    "\n",
    "Lo relevante para el análisis y entendimiento del problema es:\n",
    "\n",
    "* La información relevante será tomada a nivel de hit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ENTENDIMIENTO DE LOS DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################################\n",
    "## Comenzare por importar los datos a partir del archivo (fichero) proporcionado                       ##\n",
    "## Previamente descomprimí el archivo de 23GB en un directorio contenido en la variable datafilespath  ##\n",
    "## Se requiere descomprimir todos y cada uno de los archivos a su formato txt primero                  ##\n",
    "## Se deben depositar en un solo directorio representado por la variable extractpath                   ##\n",
    "#########################################################################################################\n",
    "\n",
    "import os\n",
    "import gzip\n",
    "\n",
    "# Si se desea se pueden modificar las variables\n",
    "\n",
    "datafilespath = \"F:\\\\DATA_TFM\\\\ORIGINAL_ZIPs\" #Este path es para mi PC en el trabajo\n",
    "#datafilespath = \"U:\\MASTER INT. ARTIFICIAL\\TFM_DATA\\ORIGINAL_ZIPs\"  #Este path es para mi laptop en casa\n",
    "filesarr = os.listdir(datafilespath)\n",
    "#extractpath = \"U:\\MASTER INT. ARTIFICIAL\\TFM_DATA\\TXTs\" #Este path es para mi laptop en casa\n",
    "extractpath = \"F:\\\\DATA_TFM\\\\TXTs\" #Este path es para mi PC en el trabajo\n",
    "\n",
    "\n",
    "print(\"Numero de archivos a tratar:\", len(filesarr)) #Hay que restar los directorios (TXTs y LIMPIEZA)\n",
    "print(\"Directorio en donde se extraeran-extrajeron los archivos:\\n\", extractpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Los archivos zip originales están en:\",datafilespath)\n",
    "print(\"El directorio donde extraeré/extraje los zips es:\",extractpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Referencias para el tratamiento con gzip\n",
    "\n",
    "* https://docs.python.org/3/library/gzip.html\n",
    "\n",
    "* https://pymotw.com/2/gzip/\n",
    "\n",
    "* https://stackoverflow.com/questions/41270130/how-to-decompress-a-file-using-python-3-x-and-gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "### ESTA CELDA SOLO DEBE EJECUTARSE UNA VEZ SI Y SOLO SI NO SE HA ###\n",
    "###       EJECUTADO PREVIAMENTE, POR FAVOR TENER CUIDADO          ###\n",
    "### EN GENERAL NO DEBERÍA EJECUTARSE SOLO SE PONE COMO REFERENCIA ###\n",
    "#####################################################################\n",
    "\n",
    "# El siguiente código, nos sirve para descomprimir uno por uno\n",
    "# todos los archivos despues de haberlos descomprimido del original de 23 GB\n",
    "# ya que en el de 23GB venian comprimidos en formato gzip y se requiere una\n",
    "# doble \"descompresión\".\n",
    "\n",
    "# Es importante mencionar que esto me resulto más recomendable hacerlo\n",
    "# en un directorio local ya que son 2555 archivos\n",
    "# Adicionalmente el tiempo que se requiere para esta descompresión, es\n",
    "# significativo, de alrededor de 30 min y es intensivo sobre todo en CPU.\n",
    "\n",
    "for i in range(len(filesarr)):\n",
    "    archivo = datafilespath+\"\\\\\"+filesarr[i]\n",
    "    print(\"Ruta y archivo a descomprimir:\\n\", archivo)\n",
    "    # Con lo siguiente leemos el contenido del archivo a descomprimir\n",
    "    inF = gzip.open(archivo, 'rt') #Extracción como texto no binario\n",
    "    datos = inF.read()\n",
    "    inF.close()\n",
    "    # Quitamos la extensión gz y ponemos el nombre correcto al archivo\n",
    "    nombre = filesarr[i]\n",
    "    nombredesc = nombre[:-3]\n",
    "    # Escribimos el contenido al archivo ya descomprimido\n",
    "    salida = extractpath+\"\\\\\"+nombredesc\n",
    "    output = open(salida, 'wt')\n",
    "    output.write(datos)\n",
    "    output.close()\n",
    "    print(\"\\nTerminado el archivo:\", nombredesc)\n",
    "    \n",
    "## Se debe verificar en la ruta extractpath que estén todos los archivos\n",
    "\n",
    "### Referencia\n",
    "### http://xahlee.info/python/gzip.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTAS DESPUÉS DE LA DESCOMPRESIÓN DE LOS ARCHIVOS:**\n",
    "\n",
    "* Noto que existen \"pares de archivos\", es decir siempre hay uno con denominación \"anumu\" y otro con \"numu\" y tienen el mismo número.\n",
    "* Despues de la descompresión, encuentro que existen 7 archivos los cuales tienen longitud 0 y no contienen data, también existen 5 de longitud de 1 KB y que tienen una leyenda \"...stream error...\", estos 12 archivos no nos sirven y tendrán que ser eliminados completamente. Por lo anterior, contamos con 2542 archivos para el tratamiento de datos. Los archivos vacíos y con error, los moví a un directorio fuera llamado TXTs_vacios_y_error.\n",
    "* De cada uno de los archivos se deberá extraer los hits para su posterior tratamiento, ya que esta es la data que deberemos usar para entrenar el modelo.\n",
    "* De lo que comprendí, la salida es la data del muón y las entradas son los hits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "### ANÁLISIS DE UN PRIMER ARCHIVO TÍPICO, EL QUE TIENE LA CADENA \"anumu\" EN EL NOMBRE ###\n",
    "#########################################################################################\n",
    "\n",
    "# Nombre del archivo a analizar: MC_025800_anumu_CC_a_reco.i3.gz.txt\n",
    "\n",
    "# Necesitamos los nombres de los archivos en orden alfabetico\n",
    "txtfilesarr = os.listdir(extractpath)\n",
    "\n",
    "# Debemos extraer los datos relevantes de cada archivo, empezaremos analizando solo uno de ellos\n",
    "# aunque posteriormente podemos poner esto en un bucle, pero hasta que haya dado con un metodo\n",
    "# correcto para obtener solo los datos relevantes y \"formar\" un dataset que se pueda trabajar\n",
    "\n",
    "f = open(extractpath+\"\\\\\"+txtfilesarr[0], 'rt')\n",
    "contenido = f.read()\n",
    "f.close\n",
    "\n",
    "## Para ir viendo como es el archivo:\n",
    "print (\"Nombre del archivo: \",txtfilesarr[0],\"\\n\")\n",
    "print (\"El contenido es de tipo: \",type(contenido),\"\\n\")\n",
    "print (\"El tamaño de este primer archivo es: \", len(contenido))\n",
    "#No puedeo ver todo porque el Notebook tiene una limitante de líneas\n",
    "print (contenido[0:20000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTAS DE LO QUE OBSERVO DEL PRIMER ARCHIVO TIPICO (anumu):**\n",
    "\n",
    "* Se divide por bloques de \"Eventos\" (start_event 1, start_event 2...).\n",
    "* Cuenta en la cabecera con 9 líneas que contienen diversos datos númericos separados por espacios en blanco.\n",
    "* A partir de la décima línea empieza la data relativa a los \"hit\".\n",
    "\n",
    "* Cada evento cuenta con la siguiente información (por línea):\n",
    "\n",
    "|Línea | Comentario| Datos | No. Cols.|\n",
    "|----|---|---|---|\n",
    "| 1| Número del evento | p.ej. start_event 2| N/A |\n",
    "| 2| Datos del evento |runID, frameID, trigger_counter, otro numero?, fecha, hora| 7|\n",
    "| 3| Pesos, aunque no se a que pesos se refiere| ?| 3 |\n",
    "| 4| Datos del neutrino (nu)| X, Y, Z, X', Y', Z', energía, tipo_de_partícula| 8|\n",
    "| 5| Datos del muón (mu) | X, Y, Z, X', Y', Z', energía, tipo_de_partícula| 8|\n",
    "| 6| Datos aafit | X, Y, Z, X', Y', Z', lambda, beta| 8|\n",
    "| 7| Datos bbfit_track| X, Y, Z, X', Y', Z', factor_calidad| 7|\n",
    "| 8| Datos bbfit_bright| X, Y, Z, X', Y', Z', ?| 7 |\n",
    "| 9| Datos gridfit| X, Y, Z, X', Y', Z', ?| 7|\n",
    "|10| A partir de aqui son líneas con datos de hit, cada una con 13 columnas numéricas|\n",
    "\n",
    "* Casi al final de cada evento, existe una línea que pone \"BBFit selected pulses:\" y a continuación viene una selección de hits.\n",
    "* Finalmente existe un cadena que marca el final del evento identificada como \"end_event\".\n",
    "\n",
    "Así sucesivamente hasta acabar el archivo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Referencias para alinear tablas en Celdas Markdown y del bug que hace que todo salga a la derecha:\n",
    "\n",
    "https://stackoverflow.com/questions/21892570/ipython-notebook-align-table-to-the-left-of-cell\n",
    "\n",
    "https://github.com/jupyter/notebook/issues/3919"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "#### ANÁLISIS DEL SEGUNDO ARCHIVO TÍPICO, EL QUE TIENE LA CADENA \"numu\" EN EL NOMBRE ####\n",
    "#########################################################################################\n",
    "\n",
    "# Nombre del archivo a analizar: MC_025800_numu_CC_a_reco.i3.gz.txt\n",
    "\n",
    "# Este parecer ser el \"par\" del primer archivo con nombre MC_025800_anumu_CC_a_reco.i3.gz.txt\n",
    "\n",
    "# Debemos extraer los datos relevantes de cada archivo, continuare analizando el que parece el par\n",
    "# posteriormente podemos poner esto en un bucle, pero hasta que haya dado con un metodo\n",
    "# correcto para obtener solo los datos relevantes y \"formar\" un dataset que se pueda trabajar\n",
    "\n",
    "f = open(extractpath+\"\\\\\"+txtfilesarr[1], 'rt')\n",
    "contenido = f.read()\n",
    "f.close\n",
    "\n",
    "## Para ir viendo como es el archivo:\n",
    "print (\"Nombre del archivo: \",txtfilesarr[1],\"\\n\")\n",
    "print (\"El contenido es de tipo: \",type(contenido),\"\\n\")\n",
    "print (\"El tamaño de este primer archivo es: \", len(contenido))\n",
    "#No puedeo ver todo porque el Notebook tiene una limitante de líneas\n",
    "print (contenido[0:20000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTAS DE LO QUE OBSERVO DEL SEGUNDO ARCHIVO TIPICO (numu):**\n",
    "\n",
    "* Al igual que el anterior, se divide en los mismos \"bloques\"\n",
    "* El numero del archivo (segunda línea primera cifra), es el mismo\n",
    "* Los datos numéricos en general son diferentes.\n",
    "\n",
    "Con respecto a los nombres de los archivos y de acuerdo a lo indicado por el Director de TFM, se guardará como un label, el tipo, es decir:\n",
    "\n",
    "* numu=1\n",
    "* anumu=2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ANALIZANDO OTRO PAR DE ARCHIVOS CUALQUIERA, PERO QUE SEAN \"PAREJA\" ###\n",
    "\n",
    "f = open(extractpath+\"\\\\\"+txtfilesarr[74], 'rt')\n",
    "contenido_anumu = f.read()\n",
    "f.close\n",
    "\n",
    "f = open(extractpath+\"\\\\\"+txtfilesarr[75], 'rt')\n",
    "contenido_numu = f.read()\n",
    "f.close\n",
    "\n",
    "print (\"Nombre del archivo 1: \",txtfilesarr[74])\n",
    "print (\"Nombre del archivo 2: \",txtfilesarr[75],\"\\n\")\n",
    "\n",
    "print (\"El tamaño del archivo 1 es: \", len(contenido_anumu))\n",
    "print (\"El tamaño del archivo 2 es: \", len(contenido_numu))\n",
    "\n",
    "print (\"\\nContenido significativo del primer archivo:\")\n",
    "print (contenido_anumu[0:1000],\"\\n\",\".\\n\",\".\\n\",\".\\n\")\n",
    "print (contenido_anumu[-1000:])\n",
    "\n",
    "print (\"\\nContenido del segundo archivo:\")\n",
    "print (contenido_numu[0:1000],\"\\n\",\".\\n\",\".\\n\",\".\\n\")\n",
    "print (contenido_numu[-1000:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTAS:**\n",
    "\n",
    "* Se puede observar que en la segunda línea las columnas 1 y 4, contienen el mismo valor numérico, así como la fecha y hora.\n",
    "* Se mantiene la misma estructura, pero con valores totalmente diferentes.\n",
    "* Los nan de las líneas 7 y 8 (bbfit...), se mantienen igual 2 en la primera y 3 en la segunda.\n",
    "\n",
    "Para entender mejor el problema, es muy importante hacer la relación de los datos que vienen en los archivos, con lo que menciona el documento llamado **\"A fast algorithm for muon track reconstruction and its application to the ANTARES neutrino telescope\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>Una vez \"entendida\" la estructura de los archivos, paso a revisar las instrucciones proporcionadas por mi director de TFM, aquí es importante mencionar que seguimos en la fase 2 de la metodología CRISP-DM.</font>\n",
    "\n",
    "<font size=3><B>Extracto del correo del 10 de Junio de 2019:</font></B>\n",
    "\n",
    "**1. Metadata: nos sirve para identificar los datos**\n",
    "   \n",
    "    - runID (primer número de la segunda fila), frameID (segundo número), trigger counter (tercer número), interactionID (según el nombre del fichero, puede ser numu o anumu; esta información la puedes guardar como un label o clasificando numu como 1 y anumu como 2), weights\n",
    "   \n",
    "**2. Datos de la simulación (maestro del modelo)**\n",
    "   \n",
    "    Para el neutrino (nu) y el muon tenemos en las diferentes columnas: dirección (X Y Z, coordenadas cartesianas del vector unitario), posición (X' Y' Z'), energia, tipo_de_partícula.\n",
    "   \n",
    "    La información importante para el proyecto es la del muón porque es la que se pretende predecir.\n",
    "   \n",
    "    Se puede trabajar con azimut y zenit, o con la dirección (X Y Z). Quizás lo mejor sea pasar a dos puesto que el interés máximo está en la predicción de valores de azimut (y zenit). Esto se puede hacer mediante la transformación azimut = atan2(Y/X) valores entre -pi i pi, y zenit = acos(Z), valores entre 0 i pi.\n",
    "  \n",
    "    Como digo arriba, ésta es la información que queremos predecir en los datos reales (generalización). Por tanto los valores de azimut y zenit (transformados de X Y Z) establecidos mediante simulación, los usaremos para corregir errores en el algoritmo supervisado durante el entrenamiento+validación.\n",
    "   \n",
    "\n",
    "**3. Posibles características y/o variables de entrada a considerar**\n",
    "   \n",
    "   **3.1. Resultados de análisis de traza según metodología estándar:**\n",
    "   \n",
    "    - bbfit_track_info: usaremos los 3 primeros valores que corresponden a la estimación X Y Z (transformado a azimut y zenit como se menciona arriba), y además el último valos, que es un factor de calidad (relativo al Chi-2 del ajuste estadístico). Comprobarás que hay casos en que X e Y aparecen como nan. Estos se refieren a casos single line, en los cuales el bbfit_track es incapaz de predecir el azimut con suficiente fiabilidad. En este caso podemos proceder de dos maneras, o bien gestionamos la red de forma que no considere los términos con nan (lo cual puede resultar un tanto complejo, y quizás no del todo óptimo), o bien simplificamos el proceso estableciendo un valor relativamente alejado de aquellos con fundamento físico. Como al azimut toma valores entre -pi i pi, propondría codificar esta situación (valores nan) con un valor de azimut igual a -3pi.\n",
    "   \n",
    "    - aafit: cogeremos los 3 primeros valores X Y Z, transformando a azimut y zenit, y los dos últimos que son medidas de calidad (lambda: calidad de la reconstrucción, y beta: precisión en la dirección, o sea una medida de la incertidumbre en los ángulos azimut y zenit).\n",
    "   \n",
    "   **3.2. Datos crudos (raw data): información de los hits**\n",
    "   \n",
    "    Verás líneas con la siguiente información:\n",
    "   \n",
    "    hit número_de_hit línea_detector piso_detector módulo_óptico (hay tres, con valores 0,1,2) x' y' z' (posición estructural) x y z (coordenadas cartesianas del vector unitario orientación de los módulos ópticos) tiempo amplitud frecuencia_de_hits\n",
    "   \n",
    "    De forma similar a como se ha indicado más arriba, se sugiere que en vez de utilizar 3 variables x y z para la orientación de los módulos ópticos, se usen los ángulos azimut y zenit. La transformación es la misma, pero no se debe confundir la dirección del muón con la dirección hacía la cual los módulos ópticos están orientados. Usar dos ángulos en vez de 3 coordenadas cartesianas del vector unitario, nos hace ahorrar una variable de entrada, y reduce la confusión entre (x y z)y (x' y' z').\n",
    "   \n",
    "    La información de hits puede resultar ruidosa por lo que se sugiere proceder de la forma siguiente para considerar solo aquellos hits que garantizen una relación señal ruido suficientemente alta:\n",
    "   \n",
    "    Se considerarán únicamente los hits que aparecen más abajo en BBFit selected pulses. El problema de estos es que no aparece la información del módulo óptico (puesto a 0 en todos ellos), por lo que se deberá hacer un matching de esta información con los raw hits de arriba para determinar que módulo, o módulos ópticos (ya que puede haber más de uno!) ha detectado un hit.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\"><B>De acuerdo a lo contenido en el correo del 10 de Junio, y según lo que entendí, para AAFIT y BBFIT_TRACK, el archivo de salida deberá quedarnos con la estructura siguiente:</font></B>\n",
    "\n",
    "METADATA: runID, frameID, trigger_counter, interactionID\n",
    "\n",
    "AAFIT: azimut, zenit, lambda, beta\n",
    "\n",
    "BBFIT_TRACK: azimut, zenit, quality\n",
    "\n",
    "|runID |frameID |trigger_counter|interactionID|azimut_aafit|zenit_aafit|lambda_aafit|beta_aafit|azimut_bbfit|zenit_bbfit|quality_bbfit|\n",
    "|---|---|---|---|---|---|---|---|---|---|---|\n",
    "|25800|73557|0|2|-2.345721098|0.43408972|-5.86985596|0.13053935|-9.424777961|0.306722701|0.943145803\n",
    "\n",
    "NOTA: Es importante resaltar que los datos de azimut y zenit son transformaciones, las cuales provienen de los datos originales X, Y y Z, y utilizando las siguientes fórmulas:\n",
    "\n",
    "$$azimut = atan^2{(Y/X)}$$\n",
    "$$zenit = acos{(Z)}$$\n",
    "\n",
    "NOTAS:\n",
    "* El azimut nos dará valores entre -pi i pi, y en caso de tener nan, usaré la convención de poner un valor de -3pi.\n",
    "* El zenit nos dará valores entre 0 i pi.\n",
    "* interactionId, numu=1, anumu=2\n",
    "* En el caso de no contar con data para calcular el azimut de bbfit, pondremos el valor -3pi (-9.42)\n",
    "* Para esta celda use como ejemplo el archivo MC_025800_anumu_CC_a_reco.i3.gz.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3><B>Información del Muón</B></font>\n",
    "\n",
    "También es importante obtener la información del muón ya que es la \"salida\" y es lo que se pretende predecir.\n",
    "\n",
    "Esta salida según yo debería quedar como sigue:\n",
    "\n",
    "|runID |frameID |trigger_counter|interactionID|azimut_muon|zenit_muon|energy_muon|\n",
    "|---|---|---|---|---|---|---|\n",
    "|25800|73557|0|2|-1.78798043673113| 0.339901513444888| 15.9965| "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3><B>Para la data de los HIT, según lo que entendí deberá quedarnos como sigue:</B></font>\n",
    "    \n",
    "METADATA: runID, frameID, trigger_counter, interactionID\n",
    "\n",
    "HIT DATA: hit_number, linea_detector, piso_detector, modulo_optico, azimut_hit, zenit_hit\n",
    "\n",
    "|runID |frameID |trigger_counter|interactionID|hit_number |linea_detector|piso_detector|modulo_optico|azimut_hit|zenit_hit|\n",
    "|---|---|---|---|---|---|---|---|---|---|\n",
    "|25800|73557|0|2|1|4|19|1|0.197620491|\t2.356193385\n",
    "\n",
    "NOTAS:\n",
    "* Se tomarán unicamente los hit que aparecen debajo de \"BBFit selected pulses\", para garantizar una relación señal ruido suficientemente alta.\n",
    "* Se deberán correlacionar los hit seleccionados con los de arriba, con el objetivo de \"llenar\" el campo \"modulo_optico\", ya que aparece en 0 y no es correcto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Una vez comprendida la estructura de lo que debo obtener, procedo a la preparación (pre-procesamiento) de los datos.**\n",
    "\n",
    "**Me quedaré con la data que menciono en la celda anterior**\n",
    "\n",
    "**Por lo anterior, es hora de pasar a la fase 3 que es la de preparación de los datos.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4><B>Información relevante para la preparación de los datos</B></font>\n",
    "\n",
    "La diferencia no está en que uno usará el bbfit y el otro hits. Los dos usareis las dos cosas (además del aafit). La diferencia radica en como se usan los hits.\n",
    "\n",
    "- Lo primero es seleccionar los \"mejores\" hits, o sea los que pasen un filtro de calidad. De eso hemos hablado en emails anteriores, repasatelo y si tienes dudas comentamos. Hasta aquí los dos proyectos son igual.\n",
    "\n",
    "- Una vez pasado el filtro viene la diferencia, porque para ti todos los hits seleccionados son iguales en el sentido de que no se agrupan por evento.\n",
    "\n",
    "- Esta información de los hits se añade a las medidas de bbfit y aafit, y todas juntas serán tus variables de entrada.\n",
    "\n",
    "O sea, pasado el filtro, tu tendrás pongamos por ejemplo, 1000 hits. Estos 1000 hits pueden venir de 100 eventos, pero eso a ti te da igual. Tu número de entradas al modelo serán 1000, y para cada una de ellas tendrás información de hit, de bbfit y de aafit.\n",
    "\n",
    "En cambio, tu compañero organizará hits por eventos, y siguiendo el ejemplo de arriba, tendrá 100 entradas, cada una de ellas tendra informacion de bbfit, aafit y sobre los hits tendrás que escoger un número determinado de ellos (por ejemplo 5) al azar de forma que siempre se repitan el mismo número de entradas, que es una limitación de este tipo de modelos.\n",
    "\n",
    "Siguiendo el ejemplo, tu iras pasando entradas hit a hit, y él pasará eventos uno a uno, cada cuál con 5 hits (escogidos aleatoriamente).\n",
    "\n",
    "Espero que el ejemplo ayude a clarificar el asunto.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PREPARACIÓN DE LOS DATOS\n",
    "\n",
    "Una vez logrado el entendimiento de los datos, se procede al tratamiento y limpieza inicial; para comenzar a guardar en vectores (numpy o pandas) la información relevante, que en este caso son las líneas con la información relativa al muón y que todas empiezan con la cadena \"muón\".\n",
    "\n",
    "Es importante mencionar que quiero generar dos conjuntos de archivos (cada conjunto de 2542); el primer conjunto tendra la data relativa a la data aafit y bbfit; y el segundo conjunto contendrá la data relativa a los hits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El directorio dond estan los TXTs (archivos crudos): U:\\MASTER INT. ARTIFICIAL\\TFM_DATA\\TXTs \n",
      "\n",
      "La variable-arreglo que contiene la lista de archivos TXT: txtfilesarr\n",
      "\n",
      "Muestra de los primeros 3 archivos:\n",
      " ['MC_025800_anumu_CC_a_reco.i3.gz.txt', 'MC_025800_numu_CC_a_reco.i3.gz.txt', 'MC_025880_anumu_CC_a_reco.i3.gz.txt']\n"
     ]
    }
   ],
   "source": [
    "#Recordando algunas variables importantes y librerías que se requerirán\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import math\n",
    "\n",
    "extractpath = \"U:\\MASTER INT. ARTIFICIAL\\TFM_DATA\\TXTs\" #Este path es para mi laptop en casa\n",
    "#extractpath = \"F:\\\\DATA_TFM\\\\TXTs\" #Este path es para mi PC en el trabajo\n",
    "\n",
    "txtfilesarr = os.listdir(extractpath)\n",
    "\n",
    "print(\"El directorio dond estan los TXTs (archivos crudos):\",extractpath,\"\\n\")\n",
    "print(\"La variable-arreglo que contiene la lista de archivos TXT: txtfilesarr\\n\")\n",
    "print(\"Muestra de los primeros 3 archivos:\\n\", txtfilesarr[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3><B>IDEAS PARA HACER LA PREPARACIÓN Y LIMPIEZA DE LA DATA DE CADA ARCHIVO:</B></font>\n",
    "\n",
    "Lo que necesito es empezar a quitar las filas que no necesito, como:\n",
    "\n",
    "* weights\n",
    "* nu\n",
    "* bbfit_bright\n",
    "* gridfit\n",
    "* vacías (en blanco)\n",
    "\n",
    "Una idea que puede servir es la de generar índices para establecer en que líneas se encuentran los datos relevantes.\n",
    "Dado lo anterior se pueden establecer los siguientes índices:\n",
    "\n",
    "|Descripción del Indice| Nombre|\n",
    "|---|---|\n",
    "|Indice para remover líneas vacías (en blanco)|indices_a_quitar|\n",
    "|Indice para remover las líneas con data de nu|indices_nu|\n",
    "|Indice para remover las líneas con data bbfit_bright|indices_bbfit_bright|\n",
    "|Indice para remover líneas con data gridfit|indices_gridfit|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################\n",
    "############### CODIGO PARA REMOVER LAS LINEAS NO DESEADAS EN TODOS LOS ARCHIVOS ######################\n",
    "###############      Y GENERAR ARCHIVOS CSV NUEVOS YA SIN ESTAS LINEAS            ######################\n",
    "########################################################################################################\n",
    "\n",
    "##El dataframe con la data del archivo original en TXT es: ##### df ######\n",
    "\n",
    "### Para poder hacer que limpie todos los archivos debo generar un bucle\n",
    "\n",
    "for z in range(len(txtfilesarr)):\n",
    "    ##### ---> Abriendo el primer archivo para trabajarlo y despues generar un bucle para tratar todos <--- ####\n",
    "    f = open(extractpath+\"\\\\\"+txtfilesarr[z], 'rt')\n",
    "    datos = f.read()\n",
    "    f.close\n",
    "    df = pd.DataFrame([x.split(';') for x in datos.split('\\n')]) ## Creamos el dataframe con los saltos de línea\n",
    "    # Genero listas vacías de indices a quitar\n",
    "    indices_a_quitar = []; indices_weights = []; indices_nu = []; \n",
    "    indices_bbfit_bright = []; indices_gridfit = []\n",
    "    #Localizamos la ubicación (indices), de cada línea que queremos remover\n",
    "    for i in range(df.shape[0]):\n",
    "        tempstr = df[0][i]\n",
    "        if tempstr == '':\n",
    "            indices_a_quitar.append(i)  #Lista con indices para remover líneas vacías\n",
    "        if (\"weights\" in tempstr) == True:\n",
    "            indices_weights.append(i)   #Lista con índices para remover líneas weights\n",
    "        if (\"nu\" in tempstr) == True:\n",
    "            indices_nu.append(i)        #Lista con índices para remover líneas nu\n",
    "        if (\"bbfit_bright\" in tempstr) == True:\n",
    "            indices_bbfit_bright.append(i) #Lista con índices para remover líneas bbfit_bright\n",
    "        if (\"gridfit\" in tempstr) == True:\n",
    "            indices_gridfit.append(i)   #Lista con índices para remover líneas gridfit\n",
    "    #Removemos cada línea que no deseamos con base en los índices - No se si debo quitar weights!! y por eso lo comenté\n",
    "    for i in range(len(indices_a_quitar)):\n",
    "        df = df.drop(indices_a_quitar[i])\n",
    "    #for i in range(len(indices_weights)):\n",
    "    #    df = df.drop(indices_weights[i])\n",
    "    for i in range(len(indices_nu)):\n",
    "        df = df.drop(indices_nu[i])\n",
    "    for i in range(len(indices_bbfit_bright)):\n",
    "        df = df.drop(indices_bbfit_bright[i])\n",
    "    for i in range(len(indices_gridfit)):\n",
    "        df = df.drop(indices_gridfit[i])\n",
    "    #Regenero el indice del dataframe\n",
    "    df = df.reset_index(drop=True)\n",
    "    ### Código para remover los caracteres \"\"\\t\" de las lineas despues de \"start_event\" ###\n",
    "    ## Busqueda de líneas start_event\n",
    "    ## Genero una lista de indices donde estan\n",
    "    indices = []\n",
    "    for i in range(df.shape[0]):\n",
    "        if (\"start_event\" in str(df[0][i]))==True:\n",
    "            indices.append(i)\n",
    "    ## Limpieza de la línea que viene después de la línea start_event\n",
    "    ## estas líneas contienen la data relativa a runID, frameID, trigger_counter (3 primeros)\n",
    "    for i in range(len(indices)):\n",
    "        cleantemp = re.sub('\\t','', str(df[0][indices[i]+1])) #Remuevo el \\t del str\n",
    "        df[0][indices[i]+1] = cleantemp  \n",
    "    ###############################################################\n",
    "    ### Mandar a CSV el dataframe ya sin las líneas no deseadas ###\n",
    "    ###############################################################\n",
    "    csv_path = 'U:\\MASTER INT. ARTIFICIAL\\TFM_DATA\\CSVs\\\\' #Este es para mi laptop en casa\n",
    "    #csv_path = 'F:\\DATA_TFM\\CSVs\\\\' #Este es para la PC en mi trabajo\n",
    "    file = txtfilesarr[z][:-6]+\"csv\"\n",
    "    df.to_csv (csv_path+file, index = None, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Leyendo el contenido del directorio de los CSVs\n",
    "csvarr = os.listdir(csv_path)\n",
    "file = csv_path+csvarr[0]\n",
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Abriendo el CSV generado para su tratamiento\n",
    "data = pd.read_csv(file, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################\n",
    "### Vamos a indizar el dataframe para poder localizar las líneas ###\n",
    "###      que contienen la información que se requiere            ###\n",
    "####################################################################\n",
    "\n",
    "ind_start_ev = []\n",
    "ind_runid = []\n",
    "ind_weights = []\n",
    "ind_aafit = []\n",
    "ind_bbfit = []\n",
    "ind_bb_selp = []\n",
    "ind_endev = []\n",
    "\n",
    "for i in range(data.shape[0]):\n",
    "    tempstr = dftemp[0][i]\n",
    "    if \"start_event\" in tempstr:\n",
    "        ind_start_ev.append(i)\n",
    "    if \"UTC\" in tempstr:\n",
    "        ind_runid.append(i)\n",
    "    if \"weights\" in tempstr:\n",
    "        ind_weights.append(i)\n",
    "    if \"aafit\" in tempstr:\n",
    "        ind_aafit.append(i)\n",
    "    if \"bbfit\" in tempstr:\n",
    "        ind_bbfit.append(i)\n",
    "    if \"selected\" in tempstr:\n",
    "        ind_bb_selp.append(i)\n",
    "    if \"end_event\" in tempstr:\n",
    "        ind_endev.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Código para obtener el runid, frameid, trigger_counter e interactionID\n",
    "\n",
    "#Como el runid es identico para todos los datos en el archivo uso el primero que aparece en el dataframe\n",
    "tempstr = data[0][ind_runid[0]]\n",
    "#Divido la cadena en los valores que la componen para después extraer runid, frameid, trigger_counter\n",
    "templist = list(tempstr.split(\" \"))\n",
    "#Extrayendo las variables runid, framid, trigger_counter de este renglon---> esta data si cambia ojo!!\n",
    "runID = templist[0]; frameID = templist[1]; trigger_counter = templist [2]\n",
    "\n",
    "#Asigno 1 si el archivo es \"numu\" y 2 si es \"anumu\"\n",
    "if \"anumu\" in str(file):\n",
    "    interactionID = 2\n",
    "else:\n",
    "    interactionID = 1\n",
    "\n",
    "print(runID, frameID, trigger_counter, interactionID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Codigo para obtener la data de aafit\n",
    "## Probando solo para el primer evento y solo en el primer archivo\n",
    "\n",
    "tempstr0 = str(data[0][ind_aafit[0]])\n",
    "templist0 = list(tempstr0.split(\" \"))\n",
    "tempstr1 = str(data[0][ind_bbfit[0]])\n",
    "templist1 = list(tempstr1.split(\" \"))\n",
    "\n",
    "templist\n",
    "### Obteniendo datos para aafit\n",
    "X = float(templist0[2]); Y = float(templist0[3]); Z = float(templist0[4])\n",
    "#Calculo del azimut y zenit\n",
    "aafit_azimut = math.atan2(Y,X)\n",
    "aafit_zenit = math.acos(Z)\n",
    "#Asignación de lambda y beta\n",
    "aafit_lambda = templist[-2]\n",
    "aafit_beta = templist [-1]\n",
    "\n",
    "### Obteniendo datos para bbfit\n",
    "X = float(templist1[2]); Y = float(templist1[3]); Z = float(templist1[4])\n",
    "#Hay que verificar si X o Y son nan y asignar el valor de bbfit_azimut\n",
    "if math.isnan(X):\n",
    "    bbfit_azimut = -3*math.pi\n",
    "elif math.isnan(Y):\n",
    "    bbfit_azimut = -3*math.pi\n",
    "else:\n",
    "    bbfit_azimut = math.atna2(Y,X)\n",
    "\n",
    "\n",
    "bbfit_zenit = math.acos(Z)\n",
    "bbfit_quality = float(templist1[-1])\n",
    "\n",
    "print(aafit_azimut, aafit_zenit, aafit_lambda, aafit_beta, bbfit_azimut, bbfit_zenit, bbfit_quality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<B>Con lo anterior ya he podido al menos obtener la data de aafit y bbfit, incluyendo la transformación a azimut y zenit\n",
    "\n",
    "Sin embargo solo ha sido para una línea de un solo archivo... \n",
    "\n",
    "Tengo que ver como lograrlo para todo el archivo y posteriormente para TODOS los archivos (2542)</B>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "### OBTENIENDO LOS MEJORES HITS ###\n",
    "###################################\n",
    "\n",
    "#Leyendo el archivo csv ya sin las líneas no deseadas en un dataframe\n",
    "data = pd.read_csv(file, header=None)\n",
    "\n",
    "#Obteniendo los indices necesarios\n",
    "ind_start_ev = []; ind_runid = []; ind_weights = []; ind_aafit = []\n",
    "ind_bbfit = []; ind_bb_selp = []; ind_endev = []\n",
    "\n",
    "for i in range(data.shape[0]):\n",
    "    tempstr = dftemp[0][i]\n",
    "    if \"start_event\" in tempstr:\n",
    "        ind_start_ev.append(i)\n",
    "    if \"UTC\" in tempstr:\n",
    "        ind_runid.append(i)\n",
    "    if \"weights\" in tempstr:\n",
    "        ind_weights.append(i)\n",
    "    if \"aafit\" in tempstr:\n",
    "        ind_aafit.append(i)\n",
    "    if \"bbfit\" in tempstr:\n",
    "        ind_bbfit.append(i)\n",
    "    if \"selected\" in tempstr:\n",
    "        ind_bb_selp.append(i)\n",
    "    if \"end_event\" in tempstr:\n",
    "        ind_endev.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Código para obtener los \"mejores hits\" pero le falta la correlación ###\n",
    "lista = []\n",
    "#Recorremos\n",
    "for i in range(len(ind_bb_selp)):\n",
    "    linea_inicial = ind_bb_selp[i]+1\n",
    "    linea_final = ind_endev[i]\n",
    "    cont = linea_final - linea_inicial\n",
    "    for j in range(cont):\n",
    "        lista.append(data[0][linea_inicial])\n",
    "        linea_inicial += 1\n",
    "\n",
    "## Debemos remover los espacios dobles ya que me di cuenta que algunas lineas los tienen\n",
    "templist = []\n",
    "#Quitar espacios dobles\n",
    "for i in range(len(lista)):\n",
    "    templist.append(re.sub(\"\\s\\s+\", \" \", lista[i])) #re.sub quita los espacios dobles\n",
    "\n",
    "## Generando el dataframe de \"best_hits\"\n",
    "\n",
    "columnas = ['hit','num_hit','linea_detector','piso_detector','modulo_optico',\n",
    "           'x1','y1','z1','x','y','z','tiempo','amplitud','frec_hits']\n",
    "\n",
    "df_best_hits = pd.DataFrame([sub.split(\" \") for sub in templist],columns=columnas)\n",
    "\n",
    "df_best_hits.head(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Código para obtener los \"raw hits\" - basado en el anterior\n",
    "\n",
    "lista = []\n",
    "for i in range(len(ind_start_ev)):\n",
    "    linea_inicial = ind_bbfit[i]+1\n",
    "    linea_final = ind_bb_selp[i]\n",
    "    cont = linea_final - linea_inicial\n",
    "    for j in range(cont):\n",
    "        lista.append(data[0][linea_inicial])\n",
    "        linea_inicial += 1\n",
    "\n",
    "## Debemos remover los espacios dobles ya que me di cuenta que algunas lineas los tienen\n",
    "templist = []\n",
    "#Quitar espacios dobles\n",
    "for i in range(len(lista)):\n",
    "    templist.append(re.sub(\"\\s\\s+\", \" \", lista[i])) #re.sub quita los espacios dobles\n",
    "\n",
    "## Generando el dataframe de \"raw_hits\"\n",
    "\n",
    "columnas = ['hit','num_hit','linea_detector','piso_detector','modulo_optico',\n",
    "           'x1','y1','z1','x','y','z','tiempo','amplitud','frec_hits']\n",
    "\n",
    "df_raw_hits = pd.DataFrame([sub.split(\" \") for sub in templist],columns=columnas)\n",
    "\n",
    "df_raw_hits.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seguramente usaremos lo siguiente para hacer el matching\n",
    "df_best_hits.iloc[0:5,5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### TRATANDO DE HACER LA CORRELACION ########\n",
    "\n",
    "df_best_hits.iloc[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################    ############################################\n",
    "##### NO EJECUTAR 2 VECES SOLO SE REQUIERE UNA VEZ   ######    #### VERIFICAR SI ESTA CELDA SE REQUIERE ###\n",
    "#####       TARDA APROXIMADAMENTE 40 MINUTOS         ######    ############################################\n",
    "###########################################################\n",
    "\n",
    "### ESTE CODIGO DE ESTA CELDA GENERA LOS ARCHIVOS CONTENIENDO SOLO\n",
    "### LA DATA NUMÉRICA RELATIVA A AAFIT Y BBFIT, REQUIRIÓ INVESTIGACIÓN Y DIVERSAS PRUEBAS\n",
    "\n",
    "# Ahora debemos eliminar lo que no nos sirve de cada archivo\n",
    "# Esto generara otros 2542 archivos de texto pero ya solo con la data de AAFIT Y BBFIT_TRACK\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "f = open(extractpath+\"\\\\\"+txtfilesarr[0], 'rt')\n",
    "datos = f.read()\n",
    "f.close\n",
    "\n",
    "df = pd.DataFrame([x.split(';') for x in datos.split('\\n')]) ## Creamos el dataframe con los saltos de línea\n",
    "\n",
    "data_aafit = df[df[0].str.contains('aafit')] ## Nos quedamos solo con la data de aafit\n",
    "data_bbfit = df[df[0].str.contains('bbfit')] ## Nos quedamos solo con la data de bbfit\n",
    "\n",
    "\n",
    "#import numpy as np  # No se si se requiere, tal vez no\n",
    "\n",
    "#for i in range(len(txtfilesarr)):  ## Bucle para recorrer todos los archivos de texto (TXTs)\n",
    "#    f = open(extractpath+\"\\\\\"+txtfilesarr[i], 'rt')\n",
    "#    datos = f.read()\n",
    "#    f.close\n",
    "#    df = pd.DataFrame([x.split(';') for x in datos.split('\\n')]) ## Creamos el dataframe con los saltos de línea\n",
    "#    data_initial = df[df[0].str.contains('muon')]      ## Nos quedamos solo con las lineas de muon\n",
    "#    data_initial = data_initial.reset_index(drop=True) ## Reiniciamos los indices, tal vez no se requiere\n",
    "#    data_lista = data_initial.values.tolist()          ## Convertimos a lista\n",
    "    # Con las sig. líneas logro generar un archivo y agregar solo los datos numericos del muon\n",
    "    # ahora que esa en un bucle sirve para todos los archivos TXTs originales\n",
    "    #print(\"Trabajando el archivo: \", txtfilesarr[i], \"con: \", len(data_lista))\n",
    "#    out_name = str(txtfilesarr[i])  ## Tomo el nombre original\n",
    "#    out_name = out_name[:-6]        ## Le quito la extensión original\n",
    "    #El sig. codigo genera el archivo de salida\n",
    "#    with open(cleaningpath+\"\\\\\"+out_name+\"muon.txt\", \"w\") as text_file: ## Escribo el archivo con nueva extensión\n",
    "#        for i in range(len(data_lista)):  ## Hago un bucle para recorrer todo el string (archivo)\n",
    "#            temp = str(data_lista[i])  ## Lo convierto a una cadena usando una variable temporal\n",
    "#            temp = temp[7:]            ## Remuevo los primeros 7 caracteres que no se requieren (cadena \"muon\")\n",
    "#            temp = temp[:-2]           ## Remuevo los ultimos 2 caracteres (´])\n",
    "#            print(temp, file=text_file) ## Grabo la línea en el nuevo archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6><B> A PARTIR DE AQUI ES POSIBLE QUE CAMBIE TODO...  \n",
    "    RECORDAR QUE AUN ES TRABAJO EN PROGRESO</B></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "##### NO EJECUTAR 2 VECES SOLO SE REQUIERE UNA VEZ   ######\n",
    "#####       TARDA APROXIMADAMENTE 40 MINUTOS         ######\n",
    "###########################################################\n",
    "\n",
    "### ESTE CODIGO DE ESTA CELDA GENERA LOS ARCHIVOS CONTENIENDO SOLO\n",
    "### LA DATA NUMÉRICA DE LOS MUONES, REQUIRIÓ INVESTIGACIÓN Y DIVERSAS PRUEBAS\n",
    "\n",
    "# Ahora debemos eliminar la cadena muon y los caracteres \"['\" y \"']\" para solo dejar los datos numéricos\n",
    "# Esto generara otros 2554 archivos de texto pero ya más limpios, es decir solo con data numérica de muones\n",
    "\n",
    "import numpy as np  # No se si se requiere, tal vez no\n",
    "\n",
    "for i in range(len(txtfilesarr)):  ## Bucle para recorrer todos los archivos de texto (TXTs)\n",
    "    f = open(extractpath+\"\\\\\"+txtfilesarr[i], 'rt')\n",
    "    datos = f.read()\n",
    "    f.close\n",
    "    df = pd.DataFrame([x.split(';') for x in datos.split('\\n')]) ## Creamos el dataframe con los saltos de línea\n",
    "    data_initial = df[df[0].str.contains('muon')]      ## Nos quedamos solo con las lineas de muon\n",
    "    data_initial = data_initial.reset_index(drop=True) ## Reiniciamos los indices, tal vez no se requiere\n",
    "    data_lista = data_initial.values.tolist()          ## Convertimos a lista\n",
    "    # Con las sig. líneas logro generar un archivo y agregar solo los datos numericos del muon\n",
    "    # ahora que esa en un bucle sirve para todos los archivos TXTs originales\n",
    "    #print(\"Trabajando el archivo: \", txtfilesarr[i], \"con: \", len(data_lista))\n",
    "    out_name = str(txtfilesarr[i])  ## Tomo el nombre original\n",
    "    out_name = out_name[:-6]        ## Le quito la extensión original\n",
    "    #El sig. codigo genera el archivo de salida\n",
    "    with open(cleaningpath+\"\\\\\"+out_name+\"muon.txt\", \"w\") as text_file: ## Escribo el archivo con nueva extensión\n",
    "        for i in range(len(data_lista)):  ## Hago un bucle para recorrer todo el string (archivo)\n",
    "            temp = str(data_lista[i])  ## Lo convierto a una cadena usando una variable temporal\n",
    "            temp = temp[7:]            ## Remuevo los primeros 7 caracteres que no se requieren (cadena \"muon\")\n",
    "            temp = temp[:-2]           ## Remuevo los ultimos 2 caracteres (´])\n",
    "            print(temp, file=text_file) ## Grabo la línea en el nuevo archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################\n",
    "### ESTE CODIGO GENERA 2554 ARCHIVOS CON LIMPIEZA COMPLETA ###\n",
    "###    SI YA SE EJECUTO, NO SE DEBE VOLVER A EJECUTAR      ###\n",
    "###  AUNQUE A DIFERENCIA DE LOS PREVIOS ESTE TARDA \"POCO\"  ###\n",
    "##############################################################\n",
    "\n",
    "## Ya que para hacer la limpieza completa de todos los archivos, puedo generar una función o un for,\n",
    "## entonces por facilidad decidí utilizar un bucle\n",
    "\n",
    "colnames0 = ['X', 'Y', 'Z','X1', 'Y1', 'Z1', 'Energía', 'Tipo']\n",
    "colnames1 = ['X', 'Y', 'Z','AZIMUT', 'ZENIT']\n",
    "\n",
    "respath = datafilespath+\"\\\\RESULTS\"\n",
    "\n",
    "for i in range(len(cleanfilesarr)):\n",
    "    #Leemos el archivo y nos quedamos solo con las 3 primeras columnas (X,Y,Z)\n",
    "    #print(\"El archivo trabajado es:\",cleanfilesarr[i])\n",
    "    tempdf = pd.read_csv(cleaningpath+\"\\\\\"+cleanfilesarr[i], sep=' ',header=None, names=colnames0)\n",
    "    tempdf.drop(tempdf.columns[[3,4,5,6,7]], axis=1, inplace=True)\n",
    "    tempdfarr = tempdf.values #Convierto a numpy array para poder concatenarlo\n",
    "    #Generamos el arreglo que contendrá los valores de azimut y zenit\n",
    "    filas = tempdf.shape[0]; cols = 2\n",
    "    az_zen_arr = np.zeros((filas,cols)) # Generamos el arreglo para guardar azimut y zenit\n",
    "    for j in range(filas):\n",
    "        x = tempdfarr[j,0]; y = tempdfarr[j,1]; z = tempdfarr[j,2]\n",
    "        azimut = math.atan2(y,x) #Calculo del azimut\n",
    "        zenit  = math.acos(z)    #Calculo del zenit\n",
    "        az_zen_arr[j,0] = azimut\n",
    "        az_zen_arr[j,1] = zenit\n",
    "    #Unimos los 2 arreglos\n",
    "    resultarr = np.concatenate((tempdfarr,az_zen_arr), axis=1)\n",
    "    resdf = pd.DataFrame(data=resultarr, columns=colnames1)\n",
    "    out_name = str(cleanfilesarr[i])  ## Tomo el nombre original\n",
    "    out_name = out_name[:-8]          ## Le quito la extensión original\n",
    "    resdf.to_csv(respath+\"\\\\\"+out_name+\"result.txt\", sep=' ',index=None)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. MODELADO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

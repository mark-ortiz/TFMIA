{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabajo Fin de Master\n",
    "\n",
    "<font size=\"4\">**Universidad Internacional de Valencia**</font>\n",
    "\n",
    "<font size=\"3\">Título: “Aprendizaje profundo aplicado a física de partículas: estimación de trazas de partículas detectadas por el telescopio de neutrinos en el fondo marino.”</font>\n",
    "\n",
    "Máster en Inteligencia Artificial\n",
    "\n",
    "Director TFM: Dr. Salva Ardid\n",
    "\n",
    "Alumno: Ing. Marco Antonio Ortiz Meneses\n",
    "\n",
    "Curso Académico: 2018-2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3><B>CONTROL DE CAMBIOS:</B></font>\n",
    "\n",
    "TFM_v0 - Este archivo contiene el análisis inicial que es erróneo debido a la falta de comprensión de los datos, aunque el preprocesamiento realizado es muy valioso y puede servir.\n",
    "\n",
    "TVM_v0.1 - Este archivo pretende mejorar la limpieza de datos, ya que se cuenta con nuevo conocimiento de los mismos. Adicional se pretende obtener la data a nivel de hit, ya que lo que se obtuvo anteriormente son las \"salidas\", es decir el resultado.\n",
    "\n",
    "Si lo comprendí bien, en general la idea es poder comparar lo que se obtuvo de la trayectoría del muón vs lo que uno puede predecir utilizando aprendizaje profundo, para esto \"tal vez\" hay que comprender mejor que es el bbfit y el afit.\n",
    "\n",
    "**25/Jun**\n",
    "\n",
    "Continúo trabajando con Después de haber finalizado todas las materias y exámenes del Máster, me pongo a este TFM totalmente.\n",
    "\n",
    "Incorporé algunas celdas ya que con base a las instrucciones nuevamente enviadas por mi Director de TFM comprendí mejor lo que debo hacer en el preprocesamiento.\n",
    "\n",
    "**26/Jun**\n",
    "\n",
    "Hoy he comenzado a reordenar los índices para poder \"limpiar\" mejor el archivo y poder localizar la data que realmente se requiere.\n",
    "\n",
    "**28/Jun**\n",
    "\n",
    "Despues de un frustrante rato sin avances, logre \"indizar\" todos los archivos y generar los CSVs, los cuales tardaron 3 dias aproximadamente, tambien logré generar un algoritmo para extraer la data de aafit y bbbfit de cada archivo y ponerlo en un dataframe.\n",
    "\n",
    "**29/Jun**\n",
    "\n",
    "Después de probar muchas opciones y desechar algunas ideas locas (y tontas) como menciono en el Notebook que hice expreso para esto (GENERA_DF_AAFIT_BBFIT.ipynb), finalmente pude generar un algoritmo para obtener la data de bbfit, aafit, del muon y demas relevantes y poner toda la data en un solo dataframe para el tratamiento posterior que es el modelado.\n",
    "\n",
    "Me falta todavía hacer lo mismo y obtener la data para los hits, pero como dicen... un problema a la vez. Espero que la experiencia ganada me ayude, ya que los hits requieren una comparación que no he pensado a profundidad y creo que me puede dar un poco de problema...\n",
    "\n",
    "Otra cosa que hice fué poner a trabajar en paralelo en la generación de archivos CSV a 3 equipos (2 PC y 1 laptop), ya que iba muy lento e iba a tardar según mis cálculos más de 1 semana un solo equipo. Con lo anterior logré reducir el tiempo a sólo 3 días.\n",
    "\n",
    "Ahora ya tengo un conjunto de archivos CSV, que pueden ser tratados más fácilmente con pandas de python, solo para la data de aafit y bbfit, como dije todavía falta generar el algoritmo para los hits y por supuesto la data.\n",
    "\n",
    "Estoy también trabajando la sig. versión del Notebook TFM_v0.3, en el cual he hecho algo de limpieza de los anteriores y ya he eliminado algunas cosas que no servían o estaban de más y repetidas, y pondré el algoritmo completo de generación de data de aafit,bbfit y el muon.\n",
    "\n",
    "**30 Jun**\n",
    "\n",
    "Este dia Domingo he verificado que no se haya colgado nada de la extracción de CSVs, ya que en México es común la falta de energía eléctrica y pues eso da al traste con el trabajo, sin embargo todo bien, los 3 equipos terminaron satisfactoriamente y tengo todos los archivos que se requieren en CSV para hacer la extracción de data.\n",
    "\n",
    "Hoy pondré a trabajar el algoritmo para generar la data de aafit, bbfit y el muon. Después de algunos pruebas, estimo que esto tardará alrededor de 3 o 4 horas máximo y con un solo equipo PC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción\n",
    "\n",
    "Este es el Notebook que utilizaré para el desarrollo del TFM, y en el se contienen los aspectos técnicos relevantes para el tratamiento de los datos motivo del TFM.\n",
    "\n",
    "Como parte fundamental, seguiré la metodología CRISP-DM, la cual conlleva seis fases:\n",
    "* 1. Entendimiento del Negocio (o del Problema en este caso)\n",
    "* 2. Entendimiento de los Datos\n",
    "* 3. Preparación de los Datos\n",
    "* 4. Modelado\n",
    "* 5. Evaluación\n",
    "* 6. Despliegue-Implementación del modelo en producción\n",
    "\n",
    "**Es importante mencionar que los modelos se deben mantener y monitorear, ya que tienden a cambiar con el tiempo y a dejar de ser precisos, sobre todo si las circunstancias originales o variables cambian drásticamente.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ENTENDIMIENTO DEL PROBLEMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el punto 1 de la Metodología, leí los documentos proporcionados por mi Director de TFM y que son:\n",
    "* A fast algorithm for muon track reconstruction and its application to the ANTARES neutrino telescope\n",
    "* Nuclear Instruments and Methods in Physics Research A\n",
    "\n",
    "Una vez entendido a grandes razgos, como es que funciona el telescopio de neutrinos ANTARES y algunos detalles técnicos de operación, procedo a comenzar a analizar los archivos de datos proporcionados.\n",
    "\n",
    "Es importante para mí el mencionar que he leído varias veces los documentos ya que no es fácil entender de primera instancia todas las variables involucradas y sobre todo la física relativa a los neutrinos y otras partículas asociadas a estos.\n",
    "\n",
    "Lo relevante para el análisis y entendimiento del problema es:\n",
    "\n",
    "* La información relevante será tomada a nivel de hit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ENTENDIMIENTO DE LOS DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################################\n",
    "## Comenzare por importar los datos a partir del archivo (fichero) proporcionado                       ##\n",
    "## Previamente descomprimí el archivo de 23GB en un directorio contenido en la variable datafilespath  ##\n",
    "## Se requiere descomprimir todos y cada uno de los archivos a su formato txt primero                  ##\n",
    "## Se deben depositar en un solo directorio representado por la variable extractpath                   ##\n",
    "#########################################################################################################\n",
    "\n",
    "import os\n",
    "import gzip\n",
    "\n",
    "# Si se desea se pueden modificar las variables\n",
    "\n",
    "datafilespath = \"F:\\\\DATA_TFM\\\\ORIGINAL_ZIPs\" #Este path es para mi PC en el trabajo\n",
    "#datafilespath = \"U:\\MASTER INT. ARTIFICIAL\\TFM_DATA\\ORIGINAL_ZIPs\"  #Este path es para mi laptop en casa\n",
    "filesarr = os.listdir(datafilespath)\n",
    "#extractpath = \"U:\\MASTER INT. ARTIFICIAL\\TFM_DATA\\TXTs\" #Este path es para mi laptop en casa\n",
    "extractpath = \"F:\\\\DATA_TFM\\\\TXTs\" #Este path es para mi PC en el trabajo\n",
    "\n",
    "\n",
    "print(\"Numero de archivos a tratar:\", len(filesarr)) #Hay que restar los directorios (TXTs y LIMPIEZA)\n",
    "print(\"Directorio en donde se extraeran-extrajeron los archivos:\\n\", extractpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Los archivos zip originales están en:\",datafilespath)\n",
    "print(\"El directorio donde extraeré/extraje los zips es:\",extractpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Referencias para el tratamiento con gzip\n",
    "\n",
    "* https://docs.python.org/3/library/gzip.html\n",
    "\n",
    "* https://pymotw.com/2/gzip/\n",
    "\n",
    "* https://stackoverflow.com/questions/41270130/how-to-decompress-a-file-using-python-3-x-and-gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################\n",
    "### ESTA CELDA SOLO DEBE EJECUTARSE UNA VEZ SI Y SOLO SI NO SE HA ###\n",
    "###       EJECUTADO PREVIAMENTE, POR FAVOR TENER CUIDADO          ###\n",
    "### EN GENERAL NO DEBERÍA EJECUTARSE SOLO SE PONE COMO REFERENCIA ###\n",
    "#####################################################################\n",
    "\n",
    "# El siguiente código, nos sirve para descomprimir uno por uno\n",
    "# todos los archivos despues de haberlos descomprimido del original de 23 GB\n",
    "# ya que en el de 23GB venian comprimidos en formato gzip y se requiere una\n",
    "# doble \"descompresión\".\n",
    "\n",
    "# Es importante mencionar que esto me resulto más recomendable hacerlo\n",
    "# en un directorio local ya que son 2555 archivos\n",
    "# Adicionalmente el tiempo que se requiere para esta descompresión, es\n",
    "# significativo, de alrededor de 30 min y es intensivo sobre todo en CPU.\n",
    "\n",
    "for i in range(len(filesarr)):\n",
    "    archivo = datafilespath+\"\\\\\"+filesarr[i]\n",
    "    print(\"Ruta y archivo a descomprimir:\\n\", archivo)\n",
    "    # Con lo siguiente leemos el contenido del archivo a descomprimir\n",
    "    inF = gzip.open(archivo, 'rt') #Extracción como texto no binario\n",
    "    datos = inF.read()\n",
    "    inF.close()\n",
    "    # Quitamos la extensión gz y ponemos el nombre correcto al archivo\n",
    "    nombre = filesarr[i]\n",
    "    nombredesc = nombre[:-3]\n",
    "    # Escribimos el contenido al archivo ya descomprimido\n",
    "    salida = extractpath+\"\\\\\"+nombredesc\n",
    "    output = open(salida, 'wt')\n",
    "    output.write(datos)\n",
    "    output.close()\n",
    "    print(\"\\nTerminado el archivo:\", nombredesc)\n",
    "    \n",
    "## Se debe verificar en la ruta extractpath que estén todos los archivos\n",
    "\n",
    "### Referencia\n",
    "### http://xahlee.info/python/gzip.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTAS DESPUÉS DE LA DESCOMPRESIÓN DE LOS ARCHIVOS:**\n",
    "\n",
    "* Noto que existen \"pares de archivos\", es decir siempre hay uno con denominación \"anumu\" y otro con \"numu\" y tienen el mismo número.\n",
    "* Despues de la descompresión, encuentro que existen 7 archivos los cuales tienen longitud 0 y no contienen data, también existen 5 de longitud de 1 KB y que tienen una leyenda \"...stream error...\", estos 12 archivos no nos sirven y tendrán que ser eliminados completamente. Por lo anterior, contamos con 2542 archivos para el tratamiento de datos. Los archivos vacíos y con error, los moví a un directorio fuera llamado TXTs_vacios_y_error.\n",
    "* De cada uno de los archivos se deberá extraer los hits para su posterior tratamiento, ya que esta es la data que deberemos usar para entrenar el modelo.\n",
    "* De lo que comprendí, la salida es la data del muón y las entradas son los hits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "### ANÁLISIS DE UN PRIMER ARCHIVO TÍPICO, EL QUE TIENE LA CADENA \"anumu\" EN EL NOMBRE ###\n",
    "#########################################################################################\n",
    "\n",
    "# Nombre del archivo a analizar: MC_025800_anumu_CC_a_reco.i3.gz.txt\n",
    "\n",
    "# Necesitamos los nombres de los archivos en orden alfabetico\n",
    "txtfilesarr = os.listdir(extractpath)\n",
    "\n",
    "# Debemos extraer los datos relevantes de cada archivo, empezaremos analizando solo uno de ellos\n",
    "# aunque posteriormente podemos poner esto en un bucle, pero hasta que haya dado con un metodo\n",
    "# correcto para obtener solo los datos relevantes y \"formar\" un dataset que se pueda trabajar\n",
    "\n",
    "f = open(extractpath+\"\\\\\"+txtfilesarr[0], 'rt')\n",
    "contenido = f.read()\n",
    "f.close\n",
    "\n",
    "## Para ir viendo como es el archivo:\n",
    "print (\"Nombre del archivo: \",txtfilesarr[0],\"\\n\")\n",
    "print (\"El contenido es de tipo: \",type(contenido),\"\\n\")\n",
    "print (\"El tamaño de este primer archivo es: \", len(contenido))\n",
    "#No puedeo ver todo porque el Notebook tiene una limitante de líneas\n",
    "print (contenido[0:20000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTAS DE LO QUE OBSERVO DEL PRIMER ARCHIVO TIPICO (anumu):**\n",
    "\n",
    "* Se divide por bloques de \"Eventos\" (start_event 1, start_event 2...).\n",
    "* Cuenta en la cabecera con 9 líneas que contienen diversos datos númericos separados por espacios en blanco.\n",
    "* A partir de la décima línea empieza la data relativa a los \"hit\".\n",
    "\n",
    "* Cada evento cuenta con la siguiente información (por línea):\n",
    "\n",
    "|Línea | Comentario| Datos | No. Cols.|\n",
    "|----|---|---|---|\n",
    "| 1| Número del evento | p.ej. start_event 2| N/A |\n",
    "| 2| Datos del evento |runID, frameID, trigger_counter, otro numero?, fecha, hora| 7|\n",
    "| 3| Pesos, aunque no se a que pesos se refiere| ?| 3 |\n",
    "| 4| Datos del neutrino (nu)| X, Y, Z, X', Y', Z', energía, tipo_de_partícula| 8|\n",
    "| 5| Datos del muón (mu) | X, Y, Z, X', Y', Z', energía, tipo_de_partícula| 8|\n",
    "| 6| Datos aafit | X, Y, Z, X', Y', Z', lambda, beta| 8|\n",
    "| 7| Datos bbfit_track| X, Y, Z, X', Y', Z', factor_calidad| 7|\n",
    "| 8| Datos bbfit_bright| X, Y, Z, X', Y', Z', ?| 7 |\n",
    "| 9| Datos gridfit| X, Y, Z, X', Y', Z', ?| 7|\n",
    "|10| A partir de aqui son líneas con datos de hit, cada una con 13 columnas numéricas|\n",
    "\n",
    "* Casi al final de cada evento, existe una línea que pone \"BBFit selected pulses:\" y a continuación viene una selección de hits.\n",
    "* Finalmente existe un cadena que marca el final del evento identificada como \"end_event\".\n",
    "\n",
    "Así sucesivamente hasta acabar el archivo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Referencias para alinear tablas en Celdas Markdown y del bug que hace que todo salga a la derecha:\n",
    "\n",
    "https://stackoverflow.com/questions/21892570/ipython-notebook-align-table-to-the-left-of-cell\n",
    "\n",
    "https://github.com/jupyter/notebook/issues/3919"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "#### ANÁLISIS DEL SEGUNDO ARCHIVO TÍPICO, EL QUE TIENE LA CADENA \"numu\" EN EL NOMBRE ####\n",
    "#########################################################################################\n",
    "\n",
    "# Nombre del archivo a analizar: MC_025800_numu_CC_a_reco.i3.gz.txt\n",
    "\n",
    "# Este parecer ser el \"par\" del primer archivo con nombre MC_025800_anumu_CC_a_reco.i3.gz.txt\n",
    "\n",
    "# Debemos extraer los datos relevantes de cada archivo, continuare analizando el que parece el par\n",
    "# posteriormente podemos poner esto en un bucle, pero hasta que haya dado con un metodo\n",
    "# correcto para obtener solo los datos relevantes y \"formar\" un dataset que se pueda trabajar\n",
    "\n",
    "f = open(extractpath+\"\\\\\"+txtfilesarr[1], 'rt')\n",
    "contenido = f.read()\n",
    "f.close\n",
    "\n",
    "## Para ir viendo como es el archivo:\n",
    "print (\"Nombre del archivo: \",txtfilesarr[1],\"\\n\")\n",
    "print (\"El contenido es de tipo: \",type(contenido),\"\\n\")\n",
    "print (\"El tamaño de este primer archivo es: \", len(contenido))\n",
    "#No puedeo ver todo porque el Notebook tiene una limitante de líneas\n",
    "print (contenido[0:20000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTAS DE LO QUE OBSERVO DEL SEGUNDO ARCHIVO TIPICO (numu):**\n",
    "\n",
    "* Al igual que el anterior, se divide en los mismos \"bloques\"\n",
    "* El numero del archivo (segunda línea primera cifra), es el mismo\n",
    "* Los datos numéricos en general son diferentes.\n",
    "\n",
    "Con respecto a los nombres de los archivos y de acuerdo a lo indicado por el Director de TFM, se guardará como un label, el tipo, es decir:\n",
    "\n",
    "* numu=1\n",
    "* anumu=2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ANALIZANDO OTRO PAR DE ARCHIVOS CUALQUIERA, PERO QUE SEAN \"PAREJA\" ###\n",
    "\n",
    "f = open(extractpath+\"\\\\\"+txtfilesarr[74], 'rt')\n",
    "contenido_anumu = f.read()\n",
    "f.close\n",
    "\n",
    "f = open(extractpath+\"\\\\\"+txtfilesarr[75], 'rt')\n",
    "contenido_numu = f.read()\n",
    "f.close\n",
    "\n",
    "print (\"Nombre del archivo 1: \",txtfilesarr[74])\n",
    "print (\"Nombre del archivo 2: \",txtfilesarr[75],\"\\n\")\n",
    "\n",
    "print (\"El tamaño del archivo 1 es: \", len(contenido_anumu))\n",
    "print (\"El tamaño del archivo 2 es: \", len(contenido_numu))\n",
    "\n",
    "print (\"\\nContenido significativo del primer archivo:\")\n",
    "print (contenido_anumu[0:1000],\"\\n\",\".\\n\",\".\\n\",\".\\n\")\n",
    "print (contenido_anumu[-1000:])\n",
    "\n",
    "print (\"\\nContenido del segundo archivo:\")\n",
    "print (contenido_numu[0:1000],\"\\n\",\".\\n\",\".\\n\",\".\\n\")\n",
    "print (contenido_numu[-1000:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTAS:**\n",
    "\n",
    "* Se puede observar que en la segunda línea las columnas 1 y 4, contienen el mismo valor numérico, así como la fecha y hora.\n",
    "* Se mantiene la misma estructura, pero con valores totalmente diferentes.\n",
    "* Los nan de las líneas 7 y 8 (bbfit...), se mantienen igual 2 en la primera y 3 en la segunda.\n",
    "\n",
    "Para entender mejor el problema, es muy importante hacer la relación de los datos que vienen en los archivos, con lo que menciona el documento llamado **\"A fast algorithm for muon track reconstruction and its application to the ANTARES neutrino telescope\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instrucciones adicionales para comprender mejor los datos.\n",
    "\n",
    "<font size=4>Una vez \"entendida\" la estructura de los archivos, paso a revisar las instrucciones proporcionadas por mi director de TFM, aquí es importante mencionar que seguimos en la fase 2 de la metodología CRISP-DM.</font>\n",
    "\n",
    "<font size=3><B>Extracto del correo del 10 de Junio de 2019:</font></B>\n",
    "\n",
    "**1. Metadata: nos sirve para identificar los datos**\n",
    "   \n",
    "    - runID (primer número de la segunda fila), frameID (segundo número), trigger counter (tercer número), interactionID (según el nombre del fichero, puede ser numu o anumu; esta información la puedes guardar como un label o clasificando numu como 1 y anumu como 2), weights\n",
    "   \n",
    "**2. Datos de la simulación (maestro del modelo)**\n",
    "   \n",
    "    Para el neutrino (nu) y el muon tenemos en las diferentes columnas: dirección (X Y Z, coordenadas cartesianas del vector unitario), posición (X' Y' Z'), energia, tipo_de_partícula.\n",
    "   \n",
    "    La información importante para el proyecto es la del muón porque es la que se pretende predecir.\n",
    "   \n",
    "    Se puede trabajar con azimut y zenit, o con la dirección (X Y Z). Quizás lo mejor sea pasar a dos puesto que el interés máximo está en la predicción de valores de azimut (y zenit). Esto se puede hacer mediante la transformación azimut = atan2(Y/X) valores entre -pi i pi, y zenit = acos(Z), valores entre 0 i pi.\n",
    "  \n",
    "    Como digo arriba, ésta es la información que queremos predecir en los datos reales (generalización). Por tanto los valores de azimut y zenit (transformados de X Y Z) establecidos mediante simulación, los usaremos para corregir errores en el algoritmo supervisado durante el entrenamiento+validación.\n",
    "   \n",
    "\n",
    "**3. Posibles características y/o variables de entrada a considerar**\n",
    "   \n",
    "   **3.1. Resultados de análisis de traza según metodología estándar:**\n",
    "   \n",
    "    - bbfit_track_info: usaremos los 3 primeros valores que corresponden a la estimación X Y Z (transformado a azimut y zenit como se menciona arriba), y además el último valos, que es un factor de calidad (relativo al Chi-2 del ajuste estadístico). Comprobarás que hay casos en que X e Y aparecen como nan. Estos se refieren a casos single line, en los cuales el bbfit_track es incapaz de predecir el azimut con suficiente fiabilidad. En este caso podemos proceder de dos maneras, o bien gestionamos la red de forma que no considere los términos con nan (lo cual puede resultar un tanto complejo, y quizás no del todo óptimo), o bien simplificamos el proceso estableciendo un valor relativamente alejado de aquellos con fundamento físico. Como al azimut toma valores entre -pi i pi, propondría codificar esta situación (valores nan) con un valor de azimut igual a -3pi.\n",
    "   \n",
    "    - aafit: cogeremos los 3 primeros valores X Y Z, transformando a azimut y zenit, y los dos últimos que son medidas de calidad (lambda: calidad de la reconstrucción, y beta: precisión en la dirección, o sea una medida de la incertidumbre en los ángulos azimut y zenit).\n",
    "   \n",
    "   **3.2. Datos crudos (raw data): información de los hits**\n",
    "   \n",
    "    Verás líneas con la siguiente información:\n",
    "   \n",
    "    hit número_de_hit línea_detector piso_detector módulo_óptico (hay tres, con valores 0,1,2) x' y' z' (posición estructural) x y z (coordenadas cartesianas del vector unitario orientación de los módulos ópticos) tiempo amplitud frecuencia_de_hits\n",
    "   \n",
    "    De forma similar a como se ha indicado más arriba, se sugiere que en vez de utilizar 3 variables x y z para la orientación de los módulos ópticos, se usen los ángulos azimut y zenit. La transformación es la misma, pero no se debe confundir la dirección del muón con la dirección hacía la cual los módulos ópticos están orientados. Usar dos ángulos en vez de 3 coordenadas cartesianas del vector unitario, nos hace ahorrar una variable de entrada, y reduce la confusión entre (x y z)y (x' y' z').\n",
    "   \n",
    "    La información de hits puede resultar ruidosa por lo que se sugiere proceder de la forma siguiente para considerar solo aquellos hits que garantizen una relación señal ruido suficientemente alta:\n",
    "   \n",
    "    Se considerarán únicamente los hits que aparecen más abajo en BBFit selected pulses. El problema de estos es que no aparece la información del módulo óptico (puesto a 0 en todos ellos), por lo que se deberá hacer un matching de esta información con los raw hits de arriba para determinar que módulo, o módulos ópticos (ya que puede haber más de uno!) ha detectado un hit.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\"><B>De acuerdo a lo contenido en el correo del 10 de Junio, y según lo que entendí, para AAFIT y BBFIT_TRACK, el archivo de salida deberá quedarnos con la estructura siguiente:</font></B>\n",
    "\n",
    "METADATA: runID, frameID, trigger_counter, interactionID\n",
    "\n",
    "AAFIT: azimut, zenit, lambda, beta\n",
    "\n",
    "BBFIT_TRACK: azimut, zenit, quality\n",
    "\n",
    "|runID |frameID |trigger_counter|interactionID|azimut_aafit|zenit_aafit|lambda_aafit|beta_aafit|azimut_bbfit|zenit_bbfit|quality_bbfit|\n",
    "|---|---|---|---|---|---|---|---|---|---|---|\n",
    "|25800|73557|0|2|-2.345721098|0.43408972|-5.86985596|0.13053935|-9.424777961|0.306722701|0.943145803\n",
    "\n",
    "NOTA: Es importante resaltar que los datos de azimut y zenit son transformaciones, las cuales provienen de los datos originales X, Y y Z, y utilizando las siguientes fórmulas:\n",
    "\n",
    "$$azimut = atan^2{(Y/X)}$$\n",
    "$$zenit = acos{(Z)}$$\n",
    "\n",
    "NOTAS:\n",
    "* El azimut nos dará valores entre -pi i pi, y en caso de tener nan, usaré la convención de poner un valor de -3pi.\n",
    "* El zenit nos dará valores entre 0 i pi.\n",
    "* interactionId, numu=1, anumu=2\n",
    "* En el caso de no contar con data para calcular el azimut de bbfit, pondremos el valor -3pi (-9.42)\n",
    "* Para esta celda use como ejemplo el archivo MC_025800_anumu_CC_a_reco.i3.gz.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3><B>Información del Muón</B></font>\n",
    "\n",
    "También es importante obtener la información del muón ya que es la \"salida\" y es lo que se pretende predecir.\n",
    "\n",
    "Esta salida según yo debería quedar como sigue:\n",
    "\n",
    "|runID |frameID |trigger_counter|interactionID|azimut_muon|zenit_muon|energy_muon|\n",
    "|---|---|---|---|---|---|---|\n",
    "|25800|73557|0|2|-1.78798043673113| 0.339901513444888| 15.9965| "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=3><B>Para la data de los HIT, según lo que entendí deberá quedarnos como sigue:</B></font>\n",
    "    \n",
    "METADATA: runID, frameID, trigger_counter, interactionID\n",
    "\n",
    "HIT DATA: hit_number, linea_detector, piso_detector, modulo_optico, azimut_hit, zenit_hit\n",
    "\n",
    "|runID |frameID |trigger_counter|interactionID|hit_number |linea_detector|piso_detector|modulo_optico|azimut_hit|zenit_hit|\n",
    "|---|---|---|---|---|---|---|---|---|---|\n",
    "|25800|73557|0|2|1|4|19|1|0.197620491|\t2.356193385\n",
    "\n",
    "NOTAS:\n",
    "* Se tomarán unicamente los hit que aparecen debajo de \"BBFit selected pulses\", para garantizar una relación señal ruido suficientemente alta.\n",
    "* Se deberán correlacionar los hit seleccionados con los de arriba, con el objetivo de \"llenar\" el campo \"modulo_optico\", ya que aparece en 0 y no es correcto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Una vez comprendida la estructura de lo que debo obtener, procedo a la preparación (pre-procesamiento) de los datos.**\n",
    "\n",
    "**Me quedaré con la data que menciono en la celda anterior**\n",
    "\n",
    "**Por lo anterior, es hora de pasar a la fase 3 que es la de preparación de los datos.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4><B>Información relevante para la preparación de los datos</B></font>\n",
    "\n",
    "La diferencia no está en que uno usará el bbfit y el otro hits. Los dos usareis las dos cosas (además del aafit). La diferencia radica en como se usan los hits.\n",
    "\n",
    "- Lo primero es seleccionar los \"mejores\" hits, o sea los que pasen un filtro de calidad. De eso hemos hablado en emails anteriores, repasatelo y si tienes dudas comentamos. Hasta aquí los dos proyectos son igual.\n",
    "\n",
    "- Una vez pasado el filtro viene la diferencia, porque para ti todos los hits seleccionados son iguales en el sentido de que no se agrupan por evento.\n",
    "\n",
    "- Esta información de los hits se añade a las medidas de bbfit y aafit, y todas juntas serán tus variables de entrada.\n",
    "\n",
    "O sea, pasado el filtro, tu tendrás pongamos por ejemplo, 1000 hits. Estos 1000 hits pueden venir de 100 eventos, pero eso a ti te da igual. Tu número de entradas al modelo serán 1000, y para cada una de ellas tendrás información de hit, de bbfit y de aafit.\n",
    "\n",
    "En cambio, tu compañero organizará hits por eventos, y siguiendo el ejemplo de arriba, tendrá 100 entradas, cada una de ellas tendra informacion de bbfit, aafit y sobre los hits tendrás que escoger un número determinado de ellos (por ejemplo 5) al azar de forma que siempre se repitan el mismo número de entradas, que es una limitación de este tipo de modelos.\n",
    "\n",
    "Siguiendo el ejemplo, tu iras pasando entradas hit a hit, y él pasará eventos uno a uno, cada cuál con 5 hits (escogidos aleatoriamente).\n",
    "\n",
    "Espero que el ejemplo ayude a clarificar el asunto.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PREPARACIÓN DE LOS DATOS\n",
    "\n",
    "Una vez logrado el entendimiento de los datos, se procede al tratamiento y limpieza inicial; para comenzar a guardar en vectores (numpy o pandas) la información relevante, que en este caso son las líneas con la información relativa al muón, aafit, bbfit, hits y selected pulses.\n",
    "\n",
    "Es importante mencionar que mi idea es generar dos dataframes a partir de los archivos CSV (2542) ya \"limpios\"; el primer dataframe contendra la data relativa a la data aafit, bbfit; y el segundo conjunto contendrá la data relativa a los hits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Código para generar los archivos CSV, ya sin líneas vacías y sin dobles espacios, así como sin líneas no relevantes como gridfit.\n",
    "\n",
    "<font size=3><B>IDEAS PARA HACER LA PREPARACIÓN Y LIMPIEZA DE LA DATA DE CADA ARCHIVO:</B></font>\n",
    "\n",
    "Lo que necesito es empezar a quitar las filas que no necesito, como:\n",
    "\n",
    "* weights\n",
    "* nu\n",
    "* bbfit_bright\n",
    "* gridfit\n",
    "* vacías (en blanco)\n",
    "\n",
    "Una idea que puede servir es la de generar índices para establecer en que líneas se encuentran los datos relevantes.\n",
    "Dado lo anterior se pueden establecer los siguientes índices:\n",
    "\n",
    "|Descripción del Indice| Nombre|\n",
    "|---|---|\n",
    "|Indice para remover líneas vacías (en blanco)|indices_a_quitar|\n",
    "|Indice para remover las líneas con data de nu|indices_nu|\n",
    "|Indice para remover las líneas con data bbfit_bright|indices_bbfit_bright|\n",
    "|Indice para remover líneas con data gridfit|indices_gridfit|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################\n",
    "############### CODIGO PARA REMOVER LAS LINEAS NO DESEADAS EN TODOS LOS ARCHIVOS ######################\n",
    "###############      Y GENERAR ARCHIVOS CSV NUEVOS YA SIN ESTAS LINEAS            ######################\n",
    "########################################################################################################\n",
    "\n",
    "#### ESTO TARDA DÍAS, ASÍ QUE SE RECOMIENDA HACERLO CON AL MENOS 2 COMUTADORAS EN PARALELO\n",
    "#### EN MI CASO TARDÓ 3 DÍAS UTILIZANDO 3 EQUIPOS (2 PCs y 1 LAPTOP) ####\n",
    "\n",
    "##El dataframe con la data del archivo original en TXT es: ##### df ######\n",
    "\n",
    "### Para poder hacer que limpie todos los archivos debo generar un bucle\n",
    "inicio = 986 #Este parámetro varía para cada equipo 1 PC inició en 0, otra en 986 y la lap en 1086\n",
    "# importante mencionar que una PC lo hizo \"backwards\", es decir del ultimo archivo hacia el primero\n",
    "\n",
    "for z in range(100):#len(txtfilesarr)): #Lo cambie para que empieze en el archivo correcto\n",
    "    print(\"Archivo trabajando:\", txtfilesarr[inicio])\n",
    "    ##### ---> Abriendo el primer archivo para trabajarlo y despues generar un bucle para tratar todos <--- ####\n",
    "    f = open(extractpath+\"\\\\\"+txtfilesarr[inicio], 'rt')\n",
    "    datos = f.read()\n",
    "    f.close\n",
    "    df = pd.DataFrame([x.split(';') for x in datos.split('\\n')]) ## Creamos el dataframe con los saltos de línea\n",
    "    # Genero listas vacías de indices a quitar\n",
    "    indices_a_quitar = []; indices_weights = []; indices_nu = []; \n",
    "    indices_bbfit_bright = []; indices_gridfit = []\n",
    "    #Localizamos la ubicación (indices), de cada línea que queremos remover\n",
    "    for i in range(df.shape[0]):\n",
    "        tempstr = df[0][i]\n",
    "        if tempstr == '':\n",
    "            indices_a_quitar.append(i)  #Lista con indices para remover líneas vacías\n",
    "        if (\"weights\" in tempstr) == True:\n",
    "            indices_weights.append(i)   #Lista con índices para remover líneas weights\n",
    "        if (\"nu\" in tempstr) == True:\n",
    "            indices_nu.append(i)        #Lista con índices para remover líneas nu\n",
    "        if (\"bbfit_bright\" in tempstr) == True:\n",
    "            indices_bbfit_bright.append(i) #Lista con índices para remover líneas bbfit_bright\n",
    "        if (\"gridfit\" in tempstr) == True:\n",
    "            indices_gridfit.append(i)   #Lista con índices para remover líneas gridfit\n",
    "    #Removemos cada línea que no deseamos con base en los índices - No se si debo quitar weights!! y por eso lo comenté\n",
    "    for i in range(len(indices_a_quitar)):\n",
    "        df = df.drop(indices_a_quitar[i])\n",
    "    #for i in range(len(indices_weights)):\n",
    "    #    df = df.drop(indices_weights[i])\n",
    "    for i in range(len(indices_nu)):\n",
    "        df = df.drop(indices_nu[i])\n",
    "    for i in range(len(indices_bbfit_bright)):\n",
    "        df = df.drop(indices_bbfit_bright[i])\n",
    "    for i in range(len(indices_gridfit)):\n",
    "        df = df.drop(indices_gridfit[i])\n",
    "    #Regenero el indice del dataframe\n",
    "    df = df.reset_index(drop=True)\n",
    "    ### Código para remover los caracteres \"\"\\t\" de las lineas despues de \"start_event\" ###\n",
    "    ## Busqueda de líneas start_event\n",
    "    ## Genero una lista de indices donde estan\n",
    "    indices = []\n",
    "    for i in range(df.shape[0]):\n",
    "        if (\"start_event\" in str(df[0][i]))==True:\n",
    "            indices.append(i)\n",
    "    ## Limpieza de la línea que viene después de la línea start_event\n",
    "    ## estas líneas contienen la data relativa a runID, frameID, trigger_counter (3 primeros)\n",
    "    for i in range(len(indices)):\n",
    "        cleantemp = re.sub('\\t','', str(df[0][indices[i]+1])) #Remuevo el \\t del str\n",
    "        df[0][indices[i]+1] = cleantemp  \n",
    "    ###############################################################\n",
    "    ### Mandar a CSV el dataframe ya sin las líneas no deseadas ###\n",
    "    ###############################################################\n",
    "    csv_path = 'Z:\\MASTER INT. ARTIFICIAL\\TFM_DATA\\CSVs\\\\' #Este es para mi laptop en casa ojo cambia para la PC\n",
    "    #csv_path = 'F:\\DATA_TFM\\CSVs\\\\' #Este es para la PC en mi trabajo\n",
    "    file = txtfilesarr[inicio][:-6]+\"csv\"\n",
    "    df.to_csv (csv_path+file, index = None, header=None)\n",
    "    inicio += 1\n",
    "    \n",
    "## Los parámetros que usé para poner a trabajar la lap fueron\n",
    "## inicio = 986+100 #Ya que aquí empieza el indice del archivo, es decir la PC hizo 100 y la lap 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Código para obtener el primer dataframe con la data de aafit, bbfit y el muón.\n",
    "\n",
    "No es necesario volverlo a ejecutar, ya que ya tengo generado el archivo csv con la data correspondiente, solo lo pongo a manera de demostrar que se hizo el trabajo. Adicional, el generar el dataframe con la data lleva alrededor de 3 horas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recordando algunas variables importantes, funciones y librerías que se requerirán\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import math\n",
    "\n",
    "#extractpath = \"U:\\MASTER INT. ARTIFICIAL\\TFM_DATA\\TXTs\" #Este path es para mi laptop en casa\n",
    "#extractpath = \"F:\\\\DATA_TFM\\\\TXTs\" #Este path es para mi PC en el trabajo\n",
    "extractpath = \"Z:\\MASTER INT. ARTIFICIAL\\TFM_DATA\\TXTs\" #Este es para mi no tan nueva PC-Desktop\n",
    "\n",
    "### El sig. bloque de código comentado, lo utilicé mucho para no confundirme\n",
    "#print(\"El directorio dond estan los TXTs (archivos crudos):\",extractpath,\"\\n\")\n",
    "#print(\"La variable-arreglo que contiene la lista de archivos TXT: txtfilesarr\\n\")\n",
    "#print(\"Muestra de los primeros 3 archivos:\\n\", txtfilesarr[0:3])\n",
    "###\n",
    "\n",
    "txtfilesarr = os.listdir(extractpath); txtfilesarr=sorted(txtfilesarr)\n",
    "#csv_path = \"F:\\\\DATA_TFM\\\\CSVs\\\\\"\n",
    "csv_path = \"Z:\\MASTER INT. ARTIFICIAL\\TFM_DATA\\CSVs\\\\\"\n",
    "csvarr = os.listdir(csv_path); csvarr=sorted(csvarr)\n",
    "\n",
    "# Funcion para obtener los indices de un archivo\n",
    "def indices(df):\n",
    "    ind_startev = [];ind_runid = [];ind_weights = [];ind_muon = []\n",
    "    ind_bbfit = [];ind_bb_selp = [];ind_endev = [];#ind_aafit = []\n",
    "    for i in range(data.shape[0]):\n",
    "        tempstr = data[0][i]\n",
    "        if \"start_event\" in tempstr: ind_startev.append(i)\n",
    "        if \"UTC\" in tempstr: ind_runid.append(i)\n",
    "        if \"weights\" in tempstr: ind_weights.append(i)\n",
    "        if \"muon\" in tempstr: ind_muon.append(i)\n",
    "        #if \"aafit\" in tempstr:  ##NO LO CALCULARE!!! OBTENDRE LOS VALORES DE OTRA FORMA\n",
    "        #    ind_aafit.append(i) ##COMO TENGO EL INDICE DE BBFIT, RESTO 1 LINEA Y ASIGNO VALORES\n",
    "        if \"bbfit\" in tempstr:   ##SEGUN LA EXISTENCIA O NO DE LA LINEA!!!! EUREKA!!!\n",
    "            ind_bbfit.append(i)\n",
    "        if \"selected\" in tempstr: ind_bb_selp.append(i)\n",
    "        if \"end_event\" in tempstr: ind_endev.append(i)\n",
    "    return(ind_startev,ind_runid,ind_weights,ind_muon,ind_bbfit,ind_bb_selp,ind_endev)\n",
    "\n",
    "#Función para verificar si los indices tienen el mismo # de elementos\n",
    "def longitudes(i1,i2,i3,i4,i5,i6,i7):\n",
    "    a=len(i1);b=len(i2);c=len(i3);d=len(i4);e=len(i5);f=len(i6);g=len(i7)\n",
    "    #print(a,b,c,d,e,f,g)\n",
    "    if (a==b and b==c and c==d and d==e and e==f and f==g): return(True)\n",
    "    else: return(False)\n",
    "    return()\n",
    "\n",
    "### NOTA: Como siempre, la gran cuestión y prerrogativa humana es encontrar patrones en los datos y automatizar\n",
    "### búsquedas aún y cuando estos no se vean a simple vista o no sean fácilmente inferibles, tales como cuando \n",
    "### el patrón es precisamente eso... que NO HAY PATRÓN!!!, esto aplica para el no indice ind_aafit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################################\n",
    "###### ESTE CODIGO GENERARÁ UN DATAFRAME CON LA DATA DE BBFIT, AAFIT Y OTRAS DE \"\"TODOS\"\" LOS ARCHIVOS #####\n",
    "############################################################################################################\n",
    "\n",
    "#Se requiere un dataframe donde iremos guardando cada fila de data obtenida de cada archivo\n",
    "columnas = ['runID','frameID','trigger_counter','interactionID','aafit_azimut',\n",
    "            'aafit_zenit','aafit_lambda','aafit_beta','bbfit_azimut','bbfit_zenit','bbfit_quality',\n",
    "            'muon_azimut','muon_zenit','muon_energy'] #La data del muon la añadi el 30 Jun, lo habia \"olvidado\" XD\n",
    "df_data1 = pd.DataFrame(columns=columnas)\n",
    "\n",
    "consecutivo=0 #variable auxiliar para guardar el indice del ultimo elemento del archivo\n",
    "\n",
    "# Bucle para recorrer TODOS los archivos y obtener la data y metadata de TODOS\n",
    "for k in range(25): #len(csvarr))\n",
    "    #print(\"Ciclo\", k)\n",
    "    file = csv_path+csvarr[k]\n",
    "    print(\"El archivo trabajando es: \",file)\n",
    "    data = pd.read_csv(file, header=None) # Dataframe del archivo en tratamiento\n",
    "    ind_startev = [];ind_runid = [];ind_weights = [];ind_muon = []\n",
    "    ind_bbfit = [];ind_bb_selp = [];ind_endev = []\n",
    "\n",
    "    #Generacion de indices para cada archivo\n",
    "    ind_startev,ind_runid,ind_weights,ind_muon,ind_bbfit,ind_bb_selp,ind_endev = indices(data)\n",
    "\n",
    "    #Como el runid es identico para todos los datos en un mismo archivo, uso el primero que aparece en el dataframe\n",
    "    runID = int(list(str(data[0][ind_runid[0]]).split(\" \"))[0])\n",
    "    #Igual como el interactionID es el mismo para todos, asigno 1 si el archivo es \"numu\" y 2 si es \"anumu\"\n",
    "    interactionID = [lambda:1, lambda:2][\"anumu\" in str(file)]()\n",
    "    # Bucle para recorrer el archivo en curso y obtener la data relevante\n",
    "    print(\"Entrando al bucle para meter renglones al dataframe\") \n",
    "    for i in range(len(ind_startev)):\n",
    "        #Codigo para obtener los demas datos por cada evento y despues asignarlo a los hits\n",
    "        #Divido la cadena en los valores que la componen para después extraer frameid y trigger_counter\n",
    "        temprunid = list(data[0][ind_runid[i]].split(\" \"))\n",
    "        tempmuon = list(data[0][ind_muon[i]].split(\" \"))\n",
    "        tempbbfit = list(data[0][ind_bbfit[i]].split(\" \"))\n",
    "        #Extrayendo las variables frameid, trigger_counter de este renglon---> esta data si cambia ojo!!\n",
    "        frameID = temprunid[1]; trigger_counter = temprunid [2]\n",
    "        #Obteniendo y generando Datos para aafit\n",
    "        #Para calcular la data solo haremos la diferencia y si exista data la calculamos si no\n",
    "        #pues ponemos valores fuera de rango para saber después que no hubo data como -3pi\n",
    "        laafit = data[0][ind_bbfit[i]-1]\n",
    "        if \"aafit\" in laafit:\n",
    "            tempaafit = list(data[0][ind_bbfit[i]-1].split(\" \"))\n",
    "            aafit_azimut = math.atan2(float(tempaafit[3]),float(tempaafit[2]))\n",
    "            aafit_zenit = math.acos(float(tempaafit[4]))\n",
    "            aafit_lambda = tempaafit[-2]\n",
    "            aafit_beta = tempaafit [-1]\n",
    "        else:\n",
    "            aafit_azimut=-3*math.pi;aafit_zenit = -3*math.pi;aafit_lambda=1;aafit_beta=-1\n",
    "        #Obteniendo y generando Datos para bbfit\n",
    "        X = float(tempbbfit[2]); Y = float(tempbbfit[3]); Z = float(tempbbfit[4])\n",
    "        #Hay que verificar si X o Y son nan y asignar el valor de bbfit_azimut\n",
    "        if math.isnan(X):   bbfit_azimut = -3*math.pi\n",
    "        elif math.isnan(Y): bbfit_azimut = -3*math.pi\n",
    "        else: bbfit_azimut = math.atan2(Y,X)\n",
    "        bbfit_zenit = math.acos(Z)\n",
    "        bbfit_quality = float(tempbbfit[-1]) \n",
    "        #Obteniendo y generando la data del muon\n",
    "        X = float(tempmuon[1]); Y = float(tempmuon[2]); Z = float(tempmuon[3])\n",
    "        muon_azimut = math.atan2(Y,X)\n",
    "        muon_zenit = math.acos(Z)\n",
    "        muon_energy = float(tempmuon[-2])\n",
    "        #Añadimos la data obtenida y la generada al dataframe\n",
    "        renglon = [runID, frameID, trigger_counter, interactionID,aafit_azimut, aafit_zenit,\n",
    "                   aafit_lambda,aafit_beta, bbfit_azimut,bbfit_zenit,bbfit_quality,muon_azimut,muon_zenit,muon_energy]\n",
    "        #Añadiendo datos al dataframe.\n",
    "        df_data1.loc[i+consecutivo] = renglon\n",
    "    consecutivo = df_data1.shape[0]\n",
    "\n",
    "print(\"Fin de la extracción de la data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Código para obtener el segundo dataframe con la data de aafit, bbfit y el muón."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "### OBTENIENDO LOS MEJORES HITS ###\n",
    "###################################\n",
    "\n",
    "#Leyendo el archivo csv ya sin las líneas no deseadas en un dataframe\n",
    "data = pd.read_csv(file, header=None)\n",
    "\n",
    "#Obteniendo los indices necesarios\n",
    "ind_start_ev = []; ind_runid = []; ind_weights = []; ind_aafit = []\n",
    "ind_bbfit = []; ind_bb_selp = []; ind_endev = []\n",
    "\n",
    "for i in range(data.shape[0]):\n",
    "    tempstr = dftemp[0][i]\n",
    "    if \"start_event\" in tempstr:\n",
    "        ind_start_ev.append(i)\n",
    "    if \"UTC\" in tempstr:\n",
    "        ind_runid.append(i)\n",
    "    if \"weights\" in tempstr:\n",
    "        ind_weights.append(i)\n",
    "    if \"aafit\" in tempstr:\n",
    "        ind_aafit.append(i)\n",
    "    if \"bbfit\" in tempstr:\n",
    "        ind_bbfit.append(i)\n",
    "    if \"selected\" in tempstr:\n",
    "        ind_bb_selp.append(i)\n",
    "    if \"end_event\" in tempstr:\n",
    "        ind_endev.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "### Código para obtener los \"mejores hits\" pero le falta la correlación ###\n",
    "###\n",
    "lista = []\n",
    "#Recorremos el dataframe usando el indice ind_bb_selp apra encontrar los \"selected pulses\"\n",
    "for i in range(len(ind_bb_selp)):\n",
    "    linea_inicial = ind_bb_selp[i]+1\n",
    "    linea_final = ind_endev[i]\n",
    "    cont = linea_final - linea_inicial\n",
    "    for j in range(cont):\n",
    "        lista.append(df[0][linea_inicial])\n",
    "        linea_inicial += 1\n",
    "\n",
    "## Debemos remover los espacios dobles ya que me di cuenta que algunas lineas los tienen\n",
    "templist = []\n",
    "#Quitar espacios dobles\n",
    "for i in range(len(lista)):\n",
    "    templist.append(re.sub(\"\\s\\s+\", \" \", lista[i])) #re.sub quita los espacios dobles\n",
    "\n",
    "## Generando el dataframe de \"best_hits\"\n",
    "\n",
    "columnas = ['hit','num_hit','linea_detector','piso_detector','modulo_optico',\n",
    "           'x1','y1','z1','x','y','z','tiempo','amplitud','frec_hits']\n",
    "\n",
    "df_best_hits = pd.DataFrame([sub.split(\" \") for sub in templist],columns=columnas)\n",
    "\n",
    "#df_best_hits.head(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####\n",
    "## Código para obtener los \"raw hits\" - basado en el anterior\n",
    "#####\n",
    "\n",
    "lista = []\n",
    "for i in range(len(ind_start_ev)):\n",
    "    linea_inicial = ind_bbfit[i]+1\n",
    "    linea_final = ind_bb_selp[i]\n",
    "    cont = linea_final - linea_inicial\n",
    "    for j in range(cont):\n",
    "        lista.append(df[0][linea_inicial])\n",
    "        linea_inicial += 1\n",
    "\n",
    "## Debemos remover los espacios dobles ya que me di cuenta que algunas lineas los tienen\n",
    "templist = []\n",
    "#Quitar espacios dobles\n",
    "for i in range(len(lista)):\n",
    "    templist.append(re.sub(\"\\s\\s+\", \" \", lista[i])) #re.sub quita los espacios dobles\n",
    "\n",
    "## Generando el dataframe de \"raw_hits\"\n",
    "\n",
    "columnas = ['hit','num_hit','linea_detector','piso_detector','modulo_optico',\n",
    "           'x1','y1','z1','x','y','z','tiempo','amplitud','frec_hits']\n",
    "\n",
    "df_raw_hits = pd.DataFrame([sub.split(\" \") for sub in templist],columns=columnas)\n",
    "\n",
    "#df_raw_hits.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YA TENGO LOS DFs df_best_hits y df_raw_hits... AHORA... COMO HAGO LA CORRELACION??\n",
    "# NO ENTIENDO COMO HACER LA CORRELACION!!! PERO INTENTARE ENTENDER MEJOR EL ARCHIVO Y SU TRATAMIENTO\n",
    "\n",
    "print(len(df_raw_hits))\n",
    "#df_raw_hits.head(20)\n",
    "\n",
    "print(len(df_best_hits))\n",
    "#df_best_hits.head(20)\n",
    "\n",
    "#df_best_hits.iloc[0]\n",
    "#df_raw_hits.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seguramente usaremos lo siguiente para hacer el matching\n",
    "df_best_hits.iloc[0:5,5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### TRATANDO DE HACER LA CORRELACION ########\n",
    "\n",
    "df_best_hits.iloc[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. MODELADO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
